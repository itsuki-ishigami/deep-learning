{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b572247",
   "metadata": {},
   "source": [
    "# ニューラルネットワークモデルの作り方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd93bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5ff03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c82c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "#訓練に使用するデバイス\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "084a2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#クラスの定義\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #super()は「親クラスを参照するための関数」\n",
    "        self.flatten = nn.Flatten() #バッチ次元は残して、それ以降の次元を全部くっつけて1次元に\n",
    "        self.linear_relu_stack = nn.Sequential( #複数のレイヤーを順番に実行するためのコンテナ\n",
    "            nn.Linear(28*28, 512),              #Sequential = 「順番に」「連続して」\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) #nn.Flatten()を格納した変数なので引数を受け取れる\n",
    "        logits = self.linear_relu_stack(x) #logitsは「活性化関数をかける前の生の線形出力」\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093421d",
   "metadata": {},
   "source": [
    "## `super().__init__()` がないとどうなるか\n",
    "\n",
    "### **1. 結論**\n",
    "***`super().__init__()` を呼ばないと、PyTorchモデルとして正常に動作せず学習できない。***\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 理由**\n",
    "`nn.Module` は内部で以下の管理を行っている。\n",
    "\n",
    "- **パラメータ辞書 (`_parameters`)**  \n",
    "  モデル内の学習対象パラメータを登録する\n",
    "- **サブモジュール辞書 (`_modules`)**  \n",
    "  モデル内のレイヤー（`nn.Linear` など）を登録する\n",
    "- **バッファ辞書 (`_buffers`)**  \n",
    "  BatchNormの平均値や分散など、学習しないが保存する値を登録する\n",
    "\n",
    "***`super().__init__()` を呼ばないと、これらの辞書が初期化されないため、パラメータやモジュールが全く管理されない。***\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 実際に起こる問題**\n",
    "\n",
    "1. **`.parameters()` が空になる**  \n",
    "   → Optimizerに渡すパラメータがない → ***重みが更新されない***\n",
    "\n",
    "2. **`.to(device)` が効かない**  \n",
    "   → GPUに移動できない\n",
    "\n",
    "3. **`.train()` / `.eval()` が効かない**  \n",
    "   → DropoutやBatchNormの挙動が切り替わらない\n",
    "\n",
    "4. **`state_dict()` が空**  \n",
    "   → ***学習済みモデルの保存・読み込みができない***\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 実験コード例**\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# super().__init__() を呼ばない悪い例\n",
    "class BadModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # super().__init__() がない！\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "bad_model = BadModel()\n",
    "print(\"BadModel parameters:\", list(bad_model.parameters()))  # → []\n",
    "\n",
    "# 正しい例\n",
    "class GoodModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "good_model = GoodModel()\n",
    "print(\"GoodModel parameters:\", list(good_model.parameters()))  # → fcのパラメータが表示される\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e1d2f",
   "metadata": {},
   "source": [
    "## ダンダーメソッド（マジックメソッド）まとめ\n",
    "\n",
    "### **1. ダンダーメソッドとは**\n",
    "- **d**ouble **under**score → **dunder method**\n",
    "- 名前が `__◯◯__` の形式になっている特別なメソッド\n",
    "- Pythonが特定のタイミングで **自動的に呼び出す**\n",
    "- 例：\n",
    "  - `__init__` → インスタンス生成直後に呼ばれる\n",
    "  - `__len__` → `len(obj)` を呼んだときに実行される\n",
    "  - `__getitem__` → `obj[key]` と書いたときに実行される\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 基本的な特徴**\n",
    "- **自分で直接呼び出すことは可能**だが、通常は構文や組み込み関数を通して使う\n",
    "- 名前を `__` で囲むのは以下の理由：\n",
    "  - 通常のメソッドと区別するため\n",
    "  - 名前の衝突を避けるため\n",
    "  - 「ユーザーが直接呼ぶものではない」というサイン\n",
    "\n",
    "---\n",
    "\n",
    "### **3. よく使うダンダーメソッド一覧**\n",
    "\n",
    "| メソッド         | いつ呼ばれるか                     | 使用例 |\n",
    "|------------------|-----------------------------------|--------|\n",
    "| `__init__`       | インスタンス生成時                 | `obj = MyClass()` |\n",
    "| `__call__`       | 関数のように呼び出されたとき       | `obj()` |\n",
    "| `__len__`        | `len(obj)`                        | `len(my_obj)` |\n",
    "| `__getitem__`    | インデックスアクセス時             | `obj[i]` |\n",
    "| `__setitem__`    | インデックス代入時                 | `obj[i] = x` |\n",
    "| `__iter__`       | イテレーション開始時               | `for x in obj:` |\n",
    "| `__str__`        | `str(obj)` / `print(obj)`          | `print(my_obj)` |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 実例コード**\n",
    "```python\n",
    "class MyList:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"MyList({self.data})\"\n",
    "\n",
    "ml = MyList([1, 2, 3]) #この引数はinitの引数(data)に代入される\n",
    "print(len(ml))      # __len__ が呼ばれる → len(data)=3\n",
    "print(ml[1])        # インデックスアクセスにより、__getitem__ が呼ばれる → data[1]=2\n",
    "print(ml)           # printはオブジェクトを文字列化するので、__str__ が呼ばれる → MyList([1, 2, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b6287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2df4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict class: tensor([8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device) #この1はチャネル数ではなくバッチサイズ（として解釈される）\n",
    "logits = model(X) #logitsは(batch_size, num_classes)。今回は(1, 10)\n",
    "softmax = nn.Softmax(dim=1) #Softmaxクラス（nn.Moduleのサブクラス）からオブジェクトを作成。dim=1は__init__の引数\n",
    "pred_probab = softmax(logits) #dim=1によりそれぞれのクラスの数値にsoftmaxが適用→ただの数値が確率(probability)に\n",
    "y_pred = pred_probab.argmax(dim=1) #argmax()は最大値のインデックスを返す関数。maxは数値を返す\n",
    "print(f\"Predict class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838a637",
   "metadata": {},
   "source": [
    "## `forward()` を定義する理由\n",
    "PyTorch の `nn.Module` を継承したクラスでは、モデルの順伝播処理（入力 → 出力）を `forward()` に書く必要がある。  \n",
    "`forward()` の中で、入力テンソルがどのように変換されるかを定義する。\n",
    "\n",
    "例：\n",
    "- `nn.Flatten()` で `28x28` の画像を 1 次元ベクトルに変換\n",
    "- `nn.Sequential` で複数の線形層と活性化関数を順に適用\n",
    "- 最終的に活性化関数をかける前の出力（`logits`）を返す\n",
    "\n",
    "`forward()` を定義しないと、モデルがどのように計算されるかが決まらないため、必ず実装する必要がある。\n",
    "\n",
    "---\n",
    "\n",
    "## `forward()` を直接呼ばない理由\n",
    "`forward()` を直接呼び出すと、`nn.Module.__call__()` が行う以下の処理がスキップされる。\n",
    "- `train()` / `eval()` 状態に応じた挙動の切り替え\n",
    "- 自動微分用の勾配追跡開始\n",
    "- `forward()` 前後で登録されたフック処理の実行\n",
    "\n",
    "これらは学習や推論の正しい動作に必要なため、`forward()` を直接呼ぶのは非推奨。\n",
    "\n",
    "---\n",
    "\n",
    "## 正しい呼び出し方\n",
    "モデルを呼び出すときは、必ず `model(X)` の形にする。  \n",
    "`model(X)` は内部で `__call__()` が実行され、必要な背景処理を行った上で 自分で定義した`forward(X)` が呼ばれる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4cd364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#FashionMNISTモデルを各レイヤーレベルで確認\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04bdf4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "#nn.Flatten\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbcd1d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "#nn.Linear\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a75c882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.5859, -0.4630, -0.2106, -0.2163, -0.1700, -0.3592,  0.0604, -0.0669,\n",
      "         -0.1404, -0.0897, -0.1948, -0.1026,  0.0585,  0.2663, -0.8166, -0.2806,\n",
      "          0.3159, -0.0583,  0.5394,  0.3058],\n",
      "        [ 0.3369,  0.0271, -0.1892, -0.0128, -0.2601, -0.2365, -0.2626,  0.2252,\n",
      "         -0.1082,  0.1773, -0.0093,  0.0255, -0.3024,  0.3794, -0.8594, -0.1339,\n",
      "          0.1560,  0.0070,  0.5540,  0.5493],\n",
      "        [ 0.4989, -0.5179, -0.3275, -0.1282, -0.1387, -0.1049, -0.0212,  0.2768,\n",
      "         -0.0163,  0.1466,  0.1595,  0.0123, -0.0054,  0.2037, -0.3283,  0.3755,\n",
      "          0.3095,  0.0384,  0.6014,  0.6277]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.5859, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0604, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0585, 0.2663, 0.0000, 0.0000, 0.3159, 0.0000,\n",
      "         0.5394, 0.3058],\n",
      "        [0.3369, 0.0271, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2252, 0.0000,\n",
      "         0.1773, 0.0000, 0.0255, 0.0000, 0.3794, 0.0000, 0.0000, 0.1560, 0.0070,\n",
      "         0.5540, 0.5493],\n",
      "        [0.4989, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2768, 0.0000,\n",
      "         0.1466, 0.1595, 0.0123, 0.0000, 0.2037, 0.0000, 0.3755, 0.3095, 0.0384,\n",
      "         0.6014, 0.6277]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#nn.ReLU\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1) #分けずに一括でやってもいい\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa59d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Sequential\n",
    "seq_models = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_models(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e27ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Softmax\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2737a08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[-0.0012, -0.0010,  0.0088,  ...,  0.0281,  0.0008, -0.0099],\n",
      "        [-0.0186, -0.0079,  0.0236,  ..., -0.0207, -0.0353, -0.0142]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: tensor([ 0.0350, -0.0311], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[ 0.0385,  0.0047,  0.0317,  ...,  0.0218, -0.0332, -0.0024],\n",
      "        [-0.0036, -0.0271,  0.0302,  ...,  0.0436, -0.0134, -0.0315]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: tensor([ 0.0137, -0.0220], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[ 0.0212, -0.0014,  0.0049,  ..., -0.0079, -0.0135,  0.0126],\n",
      "        [ 0.0277,  0.0078, -0.0288,  ...,  0.0146, -0.0037,  0.0329]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values: tensor([-0.0246,  0.0347], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#モデルパラメータ\n",
    "#nn.Module を継承することで、モデルオブジェクト内で定義されたすべてのフィールドが自動的に追跡でき、\n",
    "#parameters() や named_parameters() メソッドを使って、モデルの各レイヤーのすべてのパラメータにアクセスできるようになる\n",
    "\n",
    "print(\"Model Structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-gpu-env)",
   "language": "python",
   "name": "pytorch-gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
