{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0555c4",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d70fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7134ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) #input tensor\n",
    "y = torch.zeros(3) #expected output(正解ラベル)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b8a61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7158a8a61db0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7158a8a617b0>\n"
     ]
    }
   ],
   "source": [
    "#grad_fnはそのテンソルがどの演算結果として生まれたかを表す情報\n",
    "print('Gradient function for z =', z.grad_fn) #zは加算(Add)演算で得られた\n",
    "                                              #計算グラフのノード名が AddBackward0\n",
    "print('Gradient function for loss =', loss.grad_fn) #lossはBCE-with-logits演算の結果\n",
    "                                                    #計算グラフのノード名がBinaryCrossEntropyWithLogitsBackward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b28620d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3144, 0.0742, 0.0005],\n",
      "        [0.3144, 0.0742, 0.0005],\n",
      "        [0.3144, 0.0742, 0.0005],\n",
      "        [0.3144, 0.0742, 0.0005],\n",
      "        [0.3144, 0.0742, 0.0005]])\n",
      "tensor([0.3144, 0.0742, 0.0005])\n"
     ]
    }
   ],
   "source": [
    "#gradにはそのテンソル（通常は学習パラメータ）に関する損失の勾配値が格納される\n",
    "loss.backward()\n",
    "print(w.grad)  # wに対する損失の勾配\n",
    "print(b.grad)  # bに対する損失の勾配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a98b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#勾配計算をしない方法(訓練済みモデルで推論するケースなど)\n",
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931ae5f",
   "metadata": {},
   "source": [
    "## 1. コンテキストマネージャーとは\n",
    "\n",
    "**「ある処理の前後に、決まった準備と後片付けを自動でやってくれる仕組み」**\n",
    "Pythonでは、この仕組みを使うオブジェクトを **コンテキストマネージャー** と呼びます。\n",
    "\n",
    "コンテキストマネージャーは、次の2つのメソッドを持っている必要があります：\n",
    "\n",
    "* `__enter__(self)`\n",
    "  → `with` ブロックに入るときに実行される処理（初期化・準備）\n",
    "* `__exit__(self, exc_type, exc_val, exc_tb)`\n",
    "  → `with` ブロックを抜けるときに必ず実行される処理（後片付け）\n",
    "  ※ブロック内で例外が発生しても実行される\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `with` 文とコンテキストマネージャーの関係\n",
    "\n",
    "`with` 文は、コンテキストマネージャーを呼び出して、その**前後処理**を自動で挟んでくれます。\n",
    "\n",
    "### 流れ\n",
    "\n",
    "```python\n",
    "with コンテキスト式 as 変数:\n",
    "    ブロック処理\n",
    "```\n",
    "\n",
    "実際の動作イメージ：\n",
    "\n",
    "1. `コンテキスト式.__enter__()` が呼ばれる\n",
    "   → 返り値が `変数` に代入される\n",
    "2. ブロックの処理が実行される\n",
    "3. ブロック終了時に `コンテキスト式.__exit__()` が呼ばれる\n",
    "   → 例外があっても必ず呼ばれる\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ファイル操作の例 (`with open(...) as f`)\n",
    "\n",
    "```python\n",
    "with open(\"sample.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    print(data)\n",
    "```\n",
    "\n",
    "* `open()` が返すファイルオブジェクトはコンテキストマネージャー\n",
    "* `f.__enter__()` → ファイルを開く（ハンドルを返す）\n",
    "* `f.__exit__()` → ファイルを閉じる（後片付け）\n",
    "\n",
    "### なぜ便利か？\n",
    "\n",
    "* `with` を使わない場合\n",
    "\n",
    "```python\n",
    "f = open(\"sample.txt\", \"r\")\n",
    "try:\n",
    "    data = f.read()\n",
    "finally:\n",
    "    f.close()  # 例外が出ても必ず閉じるようにする\n",
    "```\n",
    "\n",
    "* `with` を使えばこれを1行で安全に書ける\n",
    "\n",
    "---\n",
    "\n",
    "## 4. `torch.no_grad()` もコンテキストマネージャー\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # この中はautogradが無効になる\n",
    "```\n",
    "\n",
    "* `__enter__()` → autogradを一時的にOFF\n",
    "* `__exit__()` → 元のautograd設定に戻す\n",
    "\n",
    "つまり「勾配追跡のオン/オフ」を**安全に自動で切り替えるためのコンテキストマネージャー**です。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. まとめ\n",
    "\n",
    "* **コンテキストマネージャー** = 前後処理を自動化するオブジェクト\n",
    "* **`with` 文** = コンテキストマネージャーを使う構文\n",
    "* ファイル操作、勾配無効化、ロック取得・解放、DB接続管理などに広く使われる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d739466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#detach() は PyTorch のテンソルメソッドで、そのテンソルを計算グラフから切り離した新しいテンソルを返すためのもの\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach() #元のテンソルと同じ値を持つが、autograd（自動微分）の追跡対象外になる\n",
    "print(z_det.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-gpu-env)",
   "language": "python",
   "name": "pytorch-gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
