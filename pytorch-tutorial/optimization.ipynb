{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466ada51",
   "metadata": {},
   "source": [
    "# パラメータの最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4826c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17e2e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#コード準備\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b889944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハイパーパラメータ\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "617e7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最適化ループ(ハイパーパラメータを設定後、訓練で最適化のループを回すことで、モデルを最適化)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() #loss function（損失関数）の初期化、定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da5ca321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最適化手法設定(確率的勾配降下法：Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43de25f",
   "metadata": {},
   "source": [
    "訓練ループ内で、最適化（optimization）は3つのステップから構成される。\n",
    "\n",
    "[1] ``optimizer.zero_grad()``を実行し、モデルパラメータの勾配をリセットする。\n",
    "\n",
    "勾配の計算は蓄積されていくので、毎イテレーション、明示的にリセットする。\n",
    "\n",
    "<br>\n",
    "\n",
    "[2] 続いて、``loss.backwards()``を実行し、バックプロパゲーションを実行する。\n",
    "\n",
    "PyTorchは損失に対する各パラメータの偏微分の値（勾配）を求める。\n",
    "\n",
    "<br>\n",
    "\n",
    "[3] 最後に、``optimizer.step()``を実行し、各パラメータの勾配を使用してパラメータの値を調整する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最適化を実行するコードをループするtrain_loopの定義\n",
    "def train_loop(dataloader, model, loss_fn, optimaizer):\n",
    "    size = len(dataloader.dataset) #len(dataloader) だと「バッチ数」\n",
    "                                   #len(dataloader.dataset)だと「サンプル数」\n",
    "    for batch, (X, y) in enumerate(dataloader): #batch=0,1,2,3・・・（batch１個につきデータ６４個）\n",
    "        #予測と損失の計算\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y) #損失（関数）の値（要素１つ）\n",
    "\n",
    "        #バックプロパゲーション（誤差逆伝播法）\n",
    "        optimizer.zero_grad() #前回の勾配をリセット\n",
    "        loss.backward() #損失の勾配を計算（誤差逆伝播）\n",
    "        optimizer.step() #勾配を使ってパラメータを更新\n",
    "\n",
    "        #学習中に、進捗と損失を一定間隔で表示する処理\n",
    "        if batch % 100 == 0: #100バッチごとにログを表示する\n",
    "            loss =loss.item() #ログや表示用に使うだけなら、勾配情報は不要なので数値に変換して計算グラフから切り離すのが安全\n",
    "            current = batch * len(X) #len(X) → 1つのバッチに入っているサンプル数（バッチサイズ（今回だと６４））\n",
    "                                     #つまり、currentは今まで処理したサンプル数\n",
    "            print(f\"loss: {loss:.7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "#テストデータに対してモデルの性能を評価するtest_loopの定義\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader: #(X,y)が６４個ずつ返ってくる\n",
    "            pred = model(X) #pred は形が [batch_size, クラス数] のテンソル\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (\n",
    "                (pred.argmax(1) == y) #True/False の[batch_size] の整数テンソル（配列）\n",
    "                .type(torch.float) #boolを1.0/0.0にして数値として足せるようにするため\n",
    "                .sum() #sum()はテンソルの全要素（の数値）を足し合わせる関数\n",
    "                .item()\n",
    "            )\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /=size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f278cb",
   "metadata": {},
   "source": [
    "`enumerate` は **「繰り返し処理に、要素と同時にインデックス番号（カウンター）も付けてくれる関数」** 。\n",
    "`in dataloader` との違いは、**インデックス番号を取るかどうか**。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 基本的な動き\n",
    "\n",
    "普通の `for ... in ...` は要素だけを返す。\n",
    "\n",
    "```python\n",
    "fruits = [\"apple\", \"banana\", \"orange\"]\n",
    "\n",
    "for fruit in fruits:\n",
    "    print(fruit)\n",
    "```\n",
    "\n",
    "出力：\n",
    "\n",
    "```\n",
    "apple\n",
    "banana\n",
    "orange\n",
    "```\n",
    "\n",
    "インデックス番号（0, 1, 2…）は自分で数えるしかない。\n",
    "\n",
    "---\n",
    "\n",
    "`enumerate` を使うと、**インデックス番号と要素のペア**が返ってくる。\n",
    "\n",
    "```python\n",
    "for index, fruit in enumerate(fruits):\n",
    "    print(index, fruit)\n",
    "```\n",
    "\n",
    "出力：\n",
    "\n",
    "```\n",
    "0 apple\n",
    "1 banana\n",
    "2 orange\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. DataLoaderの場合\n",
    "\n",
    "`DataLoader` はミニバッチごとに `(X, y)` を返す。\n",
    "\n",
    "* `X` → 入力データ（画像など）\n",
    "* `y` → 正解ラベル\n",
    "\n",
    "普通にループすると：\n",
    "\n",
    "```python\n",
    "for (X, y) in dataloader:\n",
    "    # Xとyだけ取れる（バッチ番号はない）\n",
    "```\n",
    "\n",
    "`enumerate` を使うと：\n",
    "\n",
    "```python\n",
    "for batch, (X, y) in enumerate(dataloader):\n",
    "    # batch → 0, 1, 2,... （ミニバッチ番号）\n",
    "    # X, y  → そのバッチのデータ\n",
    "```\n",
    "\n",
    "これで「何番目のバッチか」を簡単に知ることができる。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 違いのまとめ\n",
    "\n",
    "| 書き方                                          | 返ってくる値         | 使いどころ           |\n",
    "| -------------------------------------------- | -------------- | --------------- |\n",
    "| `for X, y in dataloader`                     | データだけ          | バッチ番号が不要なとき     |\n",
    "| `for batch, (X, y) in enumerate(dataloader)` | `(バッチ番号, データ)` | ログ表示や進捗管理をしたいとき |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 補足：`enumerate`の第2引数\n",
    "\n",
    "`enumerate` はカウンターの開始値も指定できる。\n",
    "\n",
    "```python\n",
    "for batch, (X, y) in enumerate(dataloader, start=1):\n",
    "    print(batch)  # 1からスタート\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec111d00",
   "metadata": {},
   "source": [
    "## なぜtype()でテンソルの要素の型を変えられるのか\n",
    "\n",
    "確かに通常は **`dtype`** を使って型を変えるのが一般的ですが、\n",
    "PyTorch では `.type()` メソッドでも型変換ができます。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. `.type()` と `.to()` と `.dtype` の違い\n",
    "\n",
    "### `.type(dtype)`\n",
    "\n",
    "* PyTorch の **Tensorクラスのメソッド**\n",
    "* 引数に `torch.float` や `torch.int64` のような型クラスを渡すと、その型の新しいテンソルを返す\n",
    "\n",
    "```python\n",
    "x = torch.tensor([True, False])\n",
    "x_f = x.type(torch.float)  # torch.float32 に変換\n",
    "print(x_f.dtype)  # torch.float32\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `.to(dtype)`\n",
    "\n",
    "* より汎用的な変換メソッド\n",
    "* デバイス（CPU/GPU）や型をまとめて変えられる\n",
    "\n",
    "```python\n",
    "x_f = x.to(torch.float)  # dtypeだけ変更\n",
    "x_gpu = x.to(\"cuda\")     # GPUに移動\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `.dtype`\n",
    "\n",
    "* これは **属性** なので、型を取得するだけ（型は変わらない）\n",
    "\n",
    "```python\n",
    "print(x.dtype)   # torch.bool\n",
    "x.dtype = torch.float   # ❌ 書き換えはできない\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. なぜ `(pred.argmax(1) == y).type(torch.float)` で型が変わるのか\n",
    "\n",
    "* `(pred.argmax(1) == y)` の結果は **torch.bool型テンソル**\n",
    "* `.type(torch.float)` によって、新しいテンソルが作られ、全要素が True→1.0 / False→0.0 に変換される\n",
    "* これは内部で\\*\\*型キャスト（type casting）\\*\\*が行われるから\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 注意点\n",
    "\n",
    "* 新しいPyTorchコードでは `.type()` より `.to(dtype=torch.float)` や `.float()` を使う方が推奨されます\n",
    "\n",
    "```python\n",
    "(pred.argmax(1) == y).float()\n",
    "# 同じ意味で型変換（簡潔）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "つまり `.type()` でも `dtype` が変わるのは、\n",
    "**「型クラスを引数に取る型変換メソッド」だから** です。\n",
    "`dtype` 属性は単なる「型の情報」で、変換ではなく取得専用です。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20261f66",
   "metadata": {},
   "source": [
    "## Python の文字列フォーマット指定\n",
    "`>0.1f` は **Python の文字列フォーマット指定** の一部で、\n",
    "`0.1f` が「浮動小数点数を小数第1位まで表示」という意味である。\n",
    "\n",
    "具体的にはこうなる：\n",
    "\n",
    "* **`f`** → float形式（小数点付きの10進数）で表示\n",
    "* **`.1`** → 小数点以下1桁に丸める\n",
    "* **`0`** → 桁数指定のときにゼロ埋めする（ただし `0.1f` の場合は幅指定が無いので影響なし）\n",
    "\n",
    "例えば：\n",
    "\n",
    "```python\n",
    "x = 3.14159\n",
    "print(f\"{x:0.1f}\")  # -> '3.1'\n",
    "print(f\"{x:0.3f}\")  # -> '3.142'\n",
    "```\n",
    "\n",
    "もし `>` の部分も含まれているなら、それは**整列指定**。\n",
    "\n",
    "* **`>`** → 右寄せ（指定幅に合わせて右側に置く）\n",
    "\n",
    "例：\n",
    "\n",
    "```python\n",
    "x = 3.14159\n",
    "print(f\"{x:>6.1f}\")  # 幅6で右寄せ、小数1桁\n",
    "# 出力: '   3.1'  (前に半角スペース3つ)\n",
    "```\n",
    "\n",
    "つまり **`>0.1f`** は「右寄せ＋小数点以下1桁＋ゼロ埋め表示」の条件を組み合わせたフォーマットになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c130e8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.2974241 [    0/60000]\n",
      "loss: 2.2970819 [ 6400/60000]\n",
      "loss: 2.2742522 [12800/60000]\n",
      "loss: 2.2711635 [19200/60000]\n",
      "loss: 2.2713902 [25600/60000]\n",
      "loss: 2.2348692 [32000/60000]\n",
      "loss: 2.2607467 [38400/60000]\n",
      "loss: 2.2233007 [44800/60000]\n",
      "loss: 2.2340639 [51200/60000]\n",
      "loss: 2.2143164 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 0.034630 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.2164435 [    0/60000]\n",
      "loss: 2.2172394 [ 6400/60000]\n",
      "loss: 2.1476340 [12800/60000]\n",
      "loss: 2.1610136 [19200/60000]\n",
      "loss: 2.1612842 [25600/60000]\n",
      "loss: 2.0922568 [32000/60000]\n",
      "loss: 2.1457071 [38400/60000]\n",
      "loss: 2.0727930 [44800/60000]\n",
      "loss: 2.1037798 [51200/60000]\n",
      "loss: 2.0362027 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 0.031986 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.0770097 [    0/60000]\n",
      "loss: 2.0648038 [ 6400/60000]\n",
      "loss: 1.9247109 [12800/60000]\n",
      "loss: 1.9478282 [19200/60000]\n",
      "loss: 1.9693801 [25600/60000]\n",
      "loss: 1.8537853 [32000/60000]\n",
      "loss: 1.9366239 [38400/60000]\n",
      "loss: 1.8276259 [44800/60000]\n",
      "loss: 1.9073074 [51200/60000]\n",
      "loss: 1.7629758 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 0.028165 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.8830068 [    0/60000]\n",
      "loss: 1.8622751 [ 6400/60000]\n",
      "loss: 1.6484648 [12800/60000]\n",
      "loss: 1.6776204 [19200/60000]\n",
      "loss: 1.7784629 [25600/60000]\n",
      "loss: 1.6131141 [32000/60000]\n",
      "loss: 1.7153655 [38400/60000]\n",
      "loss: 1.6043414 [44800/60000]\n",
      "loss: 1.7362360 [51200/60000]\n",
      "loss: 1.5148232 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.025044 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.7284608 [    0/60000]\n",
      "loss: 1.7085748 [ 6400/60000]\n",
      "loss: 1.4526770 [12800/60000]\n",
      "loss: 1.4710617 [19200/60000]\n",
      "loss: 1.6618395 [25600/60000]\n",
      "loss: 1.4557632 [32000/60000]\n",
      "loss: 1.5519477 [38400/60000]\n",
      "loss: 1.4613432 [44800/60000]\n",
      "loss: 1.6239376 [51200/60000]\n",
      "loss: 1.3511230 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.023025 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.6205103 [    0/60000]\n",
      "loss: 1.6084920 [ 6400/60000]\n",
      "loss: 1.3257151 [12800/60000]\n",
      "loss: 1.3422399 [19200/60000]\n",
      "loss: 1.5772862 [25600/60000]\n",
      "loss: 1.3466948 [32000/60000]\n",
      "loss: 1.4405922 [38400/60000]\n",
      "loss: 1.3626522 [44800/60000]\n",
      "loss: 1.5402336 [51200/60000]\n",
      "loss: 1.2476478 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.021622 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.5354313 [    0/60000]\n",
      "loss: 1.5388035 [ 6400/60000]\n",
      "loss: 1.2324806 [12800/60000]\n",
      "loss: 1.2574583 [19200/60000]\n",
      "loss: 1.5087267 [25600/60000]\n",
      "loss: 1.2651601 [32000/60000]\n",
      "loss: 1.3609463 [38400/60000]\n",
      "loss: 1.2901762 [44800/60000]\n",
      "loss: 1.4740560 [51200/60000]\n",
      "loss: 1.1763617 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.020592 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.4658098 [    0/60000]\n",
      "loss: 1.4844759 [ 6400/60000]\n",
      "loss: 1.1612833 [12800/60000]\n",
      "loss: 1.1979479 [19200/60000]\n",
      "loss: 1.4543180 [25600/60000]\n",
      "loss: 1.2019749 [32000/60000]\n",
      "loss: 1.3015943 [38400/60000]\n",
      "loss: 1.2354435 [44800/60000]\n",
      "loss: 1.4209799 [51200/60000]\n",
      "loss: 1.1241820 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.019808 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.4075323 [    0/60000]\n",
      "loss: 1.4406831 [ 6400/60000]\n",
      "loss: 1.1046156 [12800/60000]\n",
      "loss: 1.1509519 [19200/60000]\n",
      "loss: 1.4113132 [25600/60000]\n",
      "loss: 1.1519951 [32000/60000]\n",
      "loss: 1.2567270 [38400/60000]\n",
      "loss: 1.1944031 [44800/60000]\n",
      "loss: 1.3779845 [51200/60000]\n",
      "loss: 1.0831687 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.019196 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.3600937 [    0/60000]\n",
      "loss: 1.4067842 [ 6400/60000]\n",
      "loss: 1.0588285 [12800/60000]\n",
      "loss: 1.1123594 [19200/60000]\n",
      "loss: 1.3779535 [25600/60000]\n",
      "loss: 1.1123753 [32000/60000]\n",
      "loss: 1.2222358 [38400/60000]\n",
      "loss: 1.1630334 [44800/60000]\n",
      "loss: 1.3435330 [51200/60000]\n",
      "loss: 1.0512048 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.018713 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#再定義（しなくてもよい）\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42cd35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-gpu-env)",
   "language": "python",
   "name": "pytorch-gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
